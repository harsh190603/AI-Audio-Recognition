{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\\\Users\\\\Harsh\\\\Desktop\\\\Audio Recognition Project\\\\audioset_tagging_cnn-master\\\\pytorch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Manually set the correct directory\n",
    "pytorch_path = r\"C:\\Users\\Harsh\\Desktop\\Audio Recognition Project\\audioset_tagging_cnn-master\\pytorch\"\n",
    "sys.path.insert(0, pytorch_path)  # Use insert(0, ...) to give it higher priority\n",
    "\n",
    "# Force reload the module to avoid conflicts\n",
    "import importlib\n",
    "\n",
    "# Import the correct pytorch_utils\n",
    "import pytorch_utils\n",
    "importlib.reload(pytorch_utils)  # Reload to ensure it picks up the correct file\n",
    "\n",
    "# Now try importing the functions\n",
    "from pytorch_utils import do_mixup, interpolate, pad_framewise_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping (Classname -> Index):\n",
      "{'breath': 0, 'cough': 1, 'crying': 2, 'laugh': 3, 'screaming': 4, 'sneeze': 5, 'yawn': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  11%|█         | 85/787 [06:18<52:09,  4.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 184\u001b[0m\n\u001b[0;32m    182\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    183\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 184\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# 11. Evaluate on the Test Set\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------\n",
    "# 1. Add the PANNs model directory to the system path\n",
    "# -------------------------\n",
    "pytorch_path = r\"C:\\Users\\Harsh\\Desktop\\Audio Recognition Project\\audioset_tagging_cnn-master\\pytorch\"\n",
    "sys.path.insert(0, pytorch_path)\n",
    "\n",
    "# Import models (both are available if needed)\n",
    "from models import Wavegram_Logmel_Cnn14  # (if you want to use this variant)\n",
    "from models import Cnn14\n",
    "\n",
    "# -------------------------\n",
    "# 2. Configuration and Paths\n",
    "# -------------------------\n",
    "DATASET_PATH = r\"C:\\Users\\Harsh\\Desktop\\Audio Recognition Project\\dataset\"\n",
    "\n",
    "TRAIN_METADATA_CSV = os.path.join(DATASET_PATH, \"metadata of train set.csv\")\n",
    "TEST_METADATA_CSV  = os.path.join(DATASET_PATH, \"metadata of test set.csv\")\n",
    "\n",
    "TRAIN_AUDIO_DIR = os.path.join(DATASET_PATH, \"train\")\n",
    "TEST_AUDIO_DIR  = os.path.join(DATASET_PATH, \"test\")\n",
    "\n",
    "# Since we are training from scratch, we do not use a pretrained checkpoint.\n",
    "# If you did want to load one, you’d specify its path here.\n",
    "# PRETRAINED_MODEL_PATH = r\"...\" \n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Audio Processing Parameters\n",
    "# -------------------------\n",
    "SAMPLE_RATE = 32000\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 320\n",
    "N_MELS = 64\n",
    "FMIN = 50\n",
    "FMAX = 14000\n",
    "\n",
    "# (No need to create mel_transform here because the model does that internally.)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Prepare Metadata and Create Class Mapping\n",
    "# -------------------------\n",
    "train_meta = pd.read_csv(TRAIN_METADATA_CSV)\n",
    "train_meta.columns = train_meta.columns.str.strip()\n",
    "\n",
    "# Use \"Classname\" as our label.\n",
    "classes = sorted(train_meta[\"Classname\"].unique())\n",
    "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
    "num_classes = len(classes)\n",
    "print(\"Class mapping (Classname -> Index):\")\n",
    "print(class_to_idx)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Define the Custom Dataset (Return Raw Waveform)\n",
    "# -------------------------\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata_csv, audio_dir, class_to_idx, transform=None):\n",
    "        \"\"\"\n",
    "        metadata_csv: CSV file containing at least columns \"Filename\" and \"Classname\"\n",
    "        audio_dir: Directory where audio files are stored.\n",
    "        class_to_idx: A dictionary mapping class names to integer labels.\n",
    "        transform: (Optional) extra transform for the waveform.\n",
    "        \"\"\"\n",
    "        self.metadata = pd.read_csv(metadata_csv)\n",
    "        self.metadata.columns = self.metadata.columns.str.strip()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        filename = row[\"Filename\"]\n",
    "        label = self.class_to_idx[row[\"Classname\"]]\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        # If stereo, take the first channel.\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform[0:1, :]\n",
    "        # Resample if needed.\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "        # Remove the channel dimension so that waveform is 1D.\n",
    "        waveform = waveform.squeeze(0)  # Now shape is (data_length,)\n",
    "        \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        return waveform, label\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = AudioDataset(TRAIN_METADATA_CSV, TRAIN_AUDIO_DIR, class_to_idx, transform=None)\n",
    "test_dataset = AudioDataset(TEST_METADATA_CSV, TEST_AUDIO_DIR, class_to_idx, transform=None)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Custom Collate Function for Raw Waveforms\n",
    "# -------------------------\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads raw 1D waveforms (each of shape [data_length]) in the batch along the time dimension\n",
    "    so that all tensors have the same length.\n",
    "    \"\"\"\n",
    "    waveforms, labels = zip(*batch)\n",
    "    max_length = max(waveform.shape[0] for waveform in waveforms)\n",
    "    padded_waveforms = []\n",
    "    for waveform in waveforms:\n",
    "        pad_length = max_length - waveform.shape[0]\n",
    "        # Pad the 1D tensor on the right (last dimension) with zeros.\n",
    "        padded_waveform = F.pad(waveform, (0, pad_length))\n",
    "        padded_waveforms.append(padded_waveform)\n",
    "    stacked_waveforms = torch.stack(padded_waveforms, dim=0)  # Shape: [batch_size, max_length]\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return stacked_waveforms, labels\n",
    "\n",
    "# Create DataLoaders with the custom collate function.\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Add PANNs Model Directory and Import the Model\n",
    "# -------------------------\n",
    "# (We already added the PANNs model directory above.)\n",
    "from models import Cnn14  # We use the standard Cnn14 model.\n",
    "\n",
    "# -------------------------\n",
    "# 8. Model Initialization (Train from Scratch)\n",
    "# -------------------------\n",
    "# Initialize the model with your dataset's number of classes (e.g., 7).\n",
    "model_params = {\n",
    "    \"sample_rate\": SAMPLE_RATE,\n",
    "    \"window_size\": N_FFT,\n",
    "    \"hop_size\": HOP_LENGTH,\n",
    "    \"mel_bins\": N_MELS,\n",
    "    \"fmin\": FMIN,\n",
    "    \"fmax\": FMAX,\n",
    "    \"classes_num\": num_classes  # This should be the number of unique classes in your dataset.\n",
    "}\n",
    "\n",
    "model = Cnn14(**model_params)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# 9. Loss & Optimizer\n",
    "# -------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# -------------------------\n",
    "# 10. Training Loop\n",
    "# -------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for waveforms, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        waveforms, labels = waveforms.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(waveforms)  # This returns a dictionary\n",
    "        logits = outputs[\"clipwise_output\"]  # Use the clipwise output for classification\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 11. Evaluate on the Test Set\n",
    "# -------------------------\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for waveforms, labels in test_loader:\n",
    "        waveforms, labels = waveforms.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(waveforms)\n",
    "        logits = outputs[\"clipwise_output\"]\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metadata of test set.csv', 'metadata of train set .csv', 'test', 'train', 'youtube ID vs link .TXT']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DATASET_PATH = r\"C:\\Users\\Harsh\\Desktop\\Audio Recognition Project\\dataset\"\n",
    "print(os.listdir(DATASET_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Cnn14' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCnn14\u001b[49m(\n\u001b[0;32m      2\u001b[0m     sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32000\u001b[39m,\n\u001b[0;32m      3\u001b[0m     window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m      4\u001b[0m     hop_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m320\u001b[39m,\n\u001b[0;32m      5\u001b[0m     mel_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      6\u001b[0m     fmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m      7\u001b[0m     fmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14000\u001b[39m,\n\u001b[0;32m      8\u001b[0m     classes_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWavegram_Logmel_Cnn14_mAP=0.439.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint \u001b[38;5;28;01melse\u001b[39;00m checkpoint)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Cnn14' is not defined"
     ]
    }
   ],
   "source": [
    "model = Cnn14(\n",
    "    sample_rate=32000,\n",
    "    window_size=1024,\n",
    "    hop_size=320,\n",
    "    mel_bins=64,\n",
    "    fmin=50,\n",
    "    fmax=14000,\n",
    "    classes_num=7\n",
    ")\n",
    "checkpoint = torch.load(r\"path_to\\Wavegram_Logmel_Cnn14_mAP=0.439.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model\"] if \"model\" in checkpoint else checkpoint)\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (725903655.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    CUDA_VISIBLE_DEVICES=0 python3 dataset/pytorch/finetune_template.py train \\\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE=\"Transfer_Cnn14\"\n",
    "CHECKPOINT_PATH=\"Cnn14_mAP=0.431.pth\"\n",
    "CUDA_VISIBLE_DEVICES=0 python3 pytorch/finetune_template.py train \\\n",
    "    --sample_rate=32000 \\\n",
    "    --window_size=1024 \\\n",
    "    --hop_size=320 \\\n",
    "    --mel_bins=64 \\\n",
    "    --fmin=50 \\\n",
    "    --fmax=14000 \\\n",
    "    --model_type=$MODEL_TYPE \\\n",
    "    --pretrained_checkpoint_path=$CHECKPOINT_PATH \\\n",
    "    --cuda\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
