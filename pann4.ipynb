{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping (Classname -> Index):\n",
      "{'breath': 0, 'cough': 1, 'crying': 2, 'laugh': 3, 'screaming': 4, 'sneeze': 5, 'yawn': 6}\n",
      "Loaded pretrained weights (except final classification layer) successfully.\n",
      "\n",
      "Starting 3-Fold Cross-Validation...\n",
      "\n",
      "--- Fold 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1:  35%|███▌      | 185/524 [05:43<10:29,  1.86s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 196 but got size 195 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 197\u001b[0m\n\u001b[0;32m    195\u001b[0m waveforms, labels \u001b[38;5;241m=\u001b[39m waveforms\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    196\u001b[0m optimizer_fold\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 197\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclipwise_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Use the classification logits\u001b[39;00m\n\u001b[0;32m    199\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion_fold(logits, labels)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\Desktop\\Audio Recognition Project\\venv1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\Desktop\\Audio Recognition Project\\venv1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\\\Users\\\\Harsh\\\\Desktop\\\\Audio Recognition Project\\\\audioset_tagging_cnn-master\\\\pytorch\\models.py:2400\u001b[0m, in \u001b[0;36mWavegram_Logmel_Cnn14.forward\u001b[1;34m(self, input, mixup_lambda)\u001b[0m\n\u001b[0;32m   2397\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block1(x, pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), pool_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2399\u001b[0m \u001b[38;5;66;03m# Concatenate Wavegram and Log mel spectrogram along the channel dimension\u001b[39;00m\n\u001b[1;32m-> 2400\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2402\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m   2403\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block2(x, pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), pool_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 196 but got size 195 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. Add the PANNs model directory to the system path\n",
    "# ------------------------------------------------\n",
    "# (Do not remove this line!)\n",
    "pytorch_path = r\"C:\\\\Users\\\\Harsh\\\\Desktop\\\\Audio Recognition Project\\\\audioset_tagging_cnn-master_2\\\\pytorch\"\n",
    "sys.path.insert(0, pytorch_path)\n",
    "\n",
    "# Import the Wavegram_Logmel_Cnn14 model (and Cnn14 if needed)\n",
    "from models import Wavegram_Logmel_Cnn14\n",
    "# (You can also import Cnn14 if desired: from models import Cnn14)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. Configuration and Paths\n",
    "# ------------------------------------------------\n",
    "DATASET_PATH = r\"C:\\\\Users\\\\Harsh\\Desktop\\\\Audio Recognition Project\\\\dataset\"\n",
    "PRETRAINED_MODEL_PATH = r\"C:\\\\Users\\\\Harsh\\\\Desktop\\\\Audio Recognition Project\\\\pretrained_models\\\\PANN\\\\Wavegram_Logmel_Cnn14_mAP=0.439.pth\"\n",
    "\n",
    "TRAIN_METADATA_CSV = os.path.join(DATASET_PATH, \"metadata of train set.csv\")\n",
    "TEST_METADATA_CSV  = os.path.join(DATASET_PATH, \"metadata of test set.csv\")\n",
    "\n",
    "TRAIN_AUDIO_DIR = os.path.join(DATASET_PATH, \"train\")\n",
    "TEST_AUDIO_DIR  = os.path.join(DATASET_PATH, \"test\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. Audio Processing Parameters\n",
    "# ------------------------------------------------\n",
    "SAMPLE_RATE = 32000\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 320\n",
    "N_MELS = 64\n",
    "FMIN = 50\n",
    "FMAX = 14000\n",
    "\n",
    "# The model computes its own spectrograms internally.\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. Prepare Metadata and Create Class Mapping\n",
    "# ------------------------------------------------\n",
    "train_meta = pd.read_csv(TRAIN_METADATA_CSV)\n",
    "train_meta.columns = train_meta.columns.str.strip()\n",
    "\n",
    "# Use the \"Classname\" column as our label.\n",
    "classes = sorted(train_meta[\"Classname\"].unique())\n",
    "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
    "num_classes = len(classes)\n",
    "print(\"Class mapping (Classname -> Index):\")\n",
    "print(class_to_idx)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5. Define the Custom Dataset (Return Raw Waveform)\n",
    "# ------------------------------------------------\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata_csv, audio_dir, class_to_idx, transform=None):\n",
    "        self.metadata = pd.read_csv(metadata_csv)\n",
    "        self.metadata.columns = self.metadata.columns.str.strip()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        filename = row[\"Filename\"]\n",
    "        label = self.class_to_idx[row[\"Classname\"]]\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        # If stereo, take the first channel.\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform[0:1, :]\n",
    "        # Resample if needed.\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "        # Squeeze to make the waveform 1D.\n",
    "        waveform = waveform.squeeze(0)  # shape: [data_length]\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        return waveform, label\n",
    "\n",
    "train_dataset = AudioDataset(TRAIN_METADATA_CSV, TRAIN_AUDIO_DIR, class_to_idx, transform=None)\n",
    "test_dataset = AudioDataset(TEST_METADATA_CSV, TEST_AUDIO_DIR, class_to_idx, transform=None)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6. Custom Collate Function for Raw Waveforms\n",
    "# ------------------------------------------------\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads raw 1D waveforms in the batch along the time dimension\n",
    "    so that all waveforms have the same length.\n",
    "    \"\"\"\n",
    "    waveforms, labels = zip(*batch)\n",
    "    max_length = max(waveform.shape[0] for waveform in waveforms)\n",
    "    padded_waveforms = []\n",
    "    for waveform in waveforms:\n",
    "        pad_length = max_length - waveform.shape[0]\n",
    "        padded_waveform = F.pad(waveform, (0, pad_length))\n",
    "        padded_waveforms.append(padded_waveform)\n",
    "    stacked_waveforms = torch.stack(padded_waveforms, dim=0)  # shape: [batch_size, max_length]\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return stacked_waveforms, labels\n",
    "\n",
    "train_loader_full = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 7. Define Model Parameters and Initialize Model\n",
    "# ------------------------------------------------\n",
    "# Here we use Wavegram_Logmel_Cnn14, which internally computes both wavegram and log-mel features.\n",
    "model_params = {\n",
    "    \"sample_rate\": SAMPLE_RATE,\n",
    "    \"window_size\": N_FFT,\n",
    "    \"hop_size\": HOP_LENGTH,\n",
    "    \"mel_bins\": N_MELS,\n",
    "    \"fmin\": FMIN,\n",
    "    \"fmax\": FMAX,\n",
    "    \"classes_num\": num_classes  # For your dataset (7 classes)\n",
    "}\n",
    "\n",
    "model = Wavegram_Logmel_Cnn14(**model_params)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 8. Load Pretrained Weights (Filtering Out Final Layer)\n",
    "# ------------------------------------------------\n",
    "checkpoint = torch.load(PRETRAINED_MODEL_PATH, map_location=DEVICE)\n",
    "pretrained_dict = checkpoint[\"model\"]\n",
    "\n",
    "# The checkpoint was trained on AudioSet (527 classes); filter out the final layer (\"fc_audioset\")\n",
    "filtered_dict = {k: v for k, v in pretrained_dict.items() if not k.startswith(\"fc_audioset\")}\n",
    "model_dict = model.state_dict()\n",
    "model_dict.update(filtered_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "print(\"Loaded pretrained weights (except final classification layer) successfully.\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 9. Loss and Optimizer\n",
    "# ------------------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 10. 3-Fold Cross-Validation on Training Set\n",
    "# ------------------------------------------------\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "fold_f1 = []\n",
    "fold_precision = []\n",
    "fold_recall = []\n",
    "\n",
    "print(\"\\nStarting 3-Fold Cross-Validation...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(train_dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize a new model for this fold and load the pretrained weights (except final layer)\n",
    "    model_fold = Wavegram_Logmel_Cnn14(**model_params)\n",
    "    model_fold.to(DEVICE)\n",
    "    model_fold.load_state_dict(model_dict)\n",
    "    optimizer_fold = optim.Adam(model_fold.parameters(), lr=LEARNING_RATE)\n",
    "    criterion_fold = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model_fold.train()\n",
    "        running_loss = 0.0\n",
    "        for waveforms, labels in tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}\"):\n",
    "            waveforms, labels = waveforms.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer_fold.zero_grad()\n",
    "            outputs = model_fold(waveforms)\n",
    "            logits = outputs[\"clipwise_output\"]  # Use the classification logits\n",
    "            loss = criterion_fold(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer_fold.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Fold {fold+1} Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate on the validation set for this fold.\n",
    "    model_fold.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in val_loader:\n",
    "            waveforms, labels = waveforms.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model_fold(waveforms)\n",
    "            logits = outputs[\"clipwise_output\"]\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    fold_f1.append(f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0))\n",
    "    fold_precision.append(precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0))\n",
    "    fold_recall.append(recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0))\n",
    "    print(f\"Fold {fold+1} - F1: {fold_f1[-1]:.4f}, Precision: {fold_precision[-1]:.4f}, Recall: {fold_recall[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n--- Average 3-Fold Cross-Validation Results ---\")\n",
    "print(f\"F1 Score: {np.mean(fold_f1):.4f}\")\n",
    "print(f\"Precision: {np.mean(fold_precision):.4f}\")\n",
    "print(f\"Recall: {np.mean(fold_recall):.4f}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 11. Train Final Model on Full Training Set and Evaluate on Test Set\n",
    "# ------------------------------------------------\n",
    "model_final = Wavegram_Logmel_Cnn14(**model_params)\n",
    "model_final.to(DEVICE)\n",
    "model_final.load_state_dict(model_dict)  # Load pretrained weights except final layer\n",
    "optimizer_final = optim.Adam(model_final.parameters(), lr=LEARNING_RATE)\n",
    "criterion_final = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nTraining final model on full training set...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model_final.train()\n",
    "    total_loss = 0.0\n",
    "    for waveforms, labels in tqdm(train_loader_full, desc=f\"Final Model Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        waveforms, labels = waveforms.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer_final.zero_grad()\n",
    "        outputs = model_final(waveforms)\n",
    "        logits = outputs[\"clipwise_output\"]\n",
    "        loss = criterion_final(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer_final.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader_full):.4f}\")\n",
    "\n",
    "torch.save(model_final.state_dict(), \"trained_model_2.pth\")\n",
    "print(\"Final training complete. Model saved as 'trained_model_2.pth'.\")\n",
    "\n",
    "model_final.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for waveforms, labels in test_loader:\n",
    "        waveforms, labels = waveforms.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model_final(waveforms)\n",
    "        logits = outputs[\"clipwise_output\"]\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
